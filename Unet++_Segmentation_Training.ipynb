{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNf8v8Y2OjV9W6I25CpqqNP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkotsche1/SMP-Binary-Image-Segmentation-Training/blob/main/Unet%2B%2B_Segmentation_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vicdeOB8DK3W"
      },
      "outputs": [],
      "source": [
        "!pip install segmentation-models-pytorch\n",
        "import segmentation_models_pytorch as smp\n",
        "from segmentation_models_pytorch import utils\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "import cv2\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import albumentations as A\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from albumentations.pytorch import ToTensorV2 \n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from glob import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## This is how you should be formatting the file structure ##\n",
        "\n",
        "#Code is included to augment the images by adding this such as rotation etc. which are then saved in the Train_Augment folder\n",
        "\n",
        "#Images should be in .jpg format\n",
        "\n",
        "# /content/Train\n",
        "# /content/Train/Images\n",
        "# /content/Train/Masks\n",
        "\n",
        "# /content/Train_Augment\n",
        "# /content/Train_Augment/Images\n",
        "# /content/Train_Augment/Masks\n",
        "\n",
        "# /content/Test\n",
        "# /content/Test/Images\n",
        "# /content/Test/Masks"
      ],
      "metadata": {
        "id": "yMyfxtYaDcLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Resizing Images\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "for dir in os.listdir(\"/content/Train\"):\n",
        "  for file in tqdm(os.listdir(\"/content/Train/\" + dir)):\n",
        "    try:\n",
        "      image = Image.open(\"/content/Train/\" + dir + \"/\" + file)\n",
        "      image = image.resize((512,512),Image.ANTIALIAS)\n",
        "      image.save(fp=\"/content/Train/\" + dir + \"/\" + file)\n",
        "    except:\n",
        "      print(file)\n",
        "\n",
        "for dir in tqdm(os.listdir(\"/content/Test\")):\n",
        "  for file in os.listdir(\"/content/Test/\" + dir):\n",
        "    try:\n",
        "      image = Image.open(\"/content/Test/\" + dir + \"/\" + file)\n",
        "      image = image.resize((512,512),Image.ANTIALIAS)\n",
        "      image.save(fp=\"/content/Test/\" + dir + \"/\" + file)\n",
        "    except:\n",
        "      print(file)\n",
        "\n"
      ],
      "metadata": {
        "id": "BVPZPI3xECiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import imageio\n",
        "from albumentations import HorizontalFlip, VerticalFlip, Rotate\n",
        "\n",
        "def load_data():\n",
        "  train_x = sorted(glob(os.path.join(\"/content/Train/Images/\", \"*.jpg\")))\n",
        "  train_y = sorted(glob(os.path.join(\"/content/Train/Masks/\", \"*.png\")))\n",
        "\n",
        "  return train_x, train_y\n",
        "\n",
        "def augment_data (images,masks,save_path, augment=True):\n",
        "\n",
        "  for idx, (x,y) in tqdm(enumerate(zip(images, masks)), total=len(images)):\n",
        "    #Extracting the Name of the file\n",
        "\n",
        "    name_x = x.split(\"/\")[-1].split(\".\")[0]\n",
        "    name_y = y.split(\"/\")[-1].split(\".\")[0]\n",
        "    \n",
        "    #Reading Image and Mask \n",
        "\n",
        "    x = cv2.imread(x, cv2.COLOR_BGR2RGB)\n",
        "    y = imageio.imread(y)\n",
        "\n",
        "    if augment:\n",
        "      \n",
        "      aug = HorizontalFlip(p=1.0)\n",
        "      augmented = aug(image=x, mask=y)\n",
        "      x1 = augmented[\"image\"]\n",
        "      y1 = augmented[\"mask\"]\n",
        "\n",
        "      aug = VerticalFlip(p=1.0)\n",
        "      augmented = aug(image=x, mask=y)\n",
        "      x2 = augmented[\"image\"]\n",
        "      y2 = augmented[\"mask\"]\n",
        "\n",
        "      aug = Rotate(limit=360, p=1.0)\n",
        "      augmented = aug(image=x, mask=y)\n",
        "      x3 = augmented[\"image\"]\n",
        "      y3 = augmented[\"mask\"]\n",
        "\n",
        "      X = [x, x1, x2, x3]\n",
        "      Y = [y, y1, y2, y3]\n",
        "\n",
        "      #X = [x, x1, x2]\n",
        "      #Y = [y, y1, y2]\n",
        "\n",
        "\n",
        "      # X = [x, x3]\n",
        "      # Y = [y, y3]\n",
        "\n",
        "    else:\n",
        "      \n",
        "      X = [x]\n",
        "      Y = [y]\n",
        "    \n",
        "    index = 0\n",
        "\n",
        "    for i,m in zip(X,Y):\n",
        "\n",
        "      tmp_image_name = f\"{name_x}_{index}.png\"\n",
        "      tmp_mask_name = f\"{name_y}_{index}.png\"\n",
        "\n",
        "      index = index +1\n",
        "\n",
        "      image_path = os.path.join(save_path, \"Images\", tmp_image_name)\n",
        "      mask_path = os.path.join(save_path, \"Masks\", tmp_mask_name)\n",
        "\n",
        "      cv2.imwrite(image_path, i)\n",
        "      cv2.imwrite(mask_path, m)\n",
        "\n",
        "train_x,train_y = load_data()\n",
        "augment_data(train_x, train_y, \"/content/Train_Augment/\", augment = True)"
      ],
      "metadata": {
        "id": "wCqcsvOEDyUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ROOTDIR:\n",
        "    train = \"/content/Train_Augment/Images\"\n",
        "    train_mask = \"/content/Train_Augment/Masks\"\n",
        "    test = \"/content/Test/Images\"\n",
        "    test_mask = \"/content/Test/Masks\""
      ],
      "metadata": {
        "id": "5U7VPEglEUVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_lst = os.listdir(ROOTDIR.train) # \"./train\"\n",
        "train_mask_lst = os.listdir(ROOTDIR.train_mask) # \"./train_masks\"\n",
        "\n",
        "sorted_test_img_lst = sorted(os.listdir(ROOTDIR.test))\n",
        "sorted_train_img_lst = sorted(train_img_lst)\n",
        "\n",
        "permuted_test_img_lst = np.random.permutation(np.array(sorted_test_img_lst))\n",
        "permuted_test_mask_lst = [x.replace(\".jpg\", \".png\") for x in permuted_test_img_lst]\n",
        "\n",
        "permuted_train_img_lst = np.random.permutation(np.array(sorted_train_img_lst))\n",
        "permuted_train_mask_lst = [x.replace(\".jpg\", \".png\") for x in permuted_train_img_lst]"
      ],
      "metadata": {
        "id": "te84sILhEhDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "class CFG:\n",
        "    device = \"cuda\"\n",
        "    split_pct = 0.1\n",
        "    learning_rate = 5e-4\n",
        "    batch_size = 32\n",
        "    epochs = 4"
      ],
      "metadata": {
        "id": "owEFPbJEFPu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_list = permuted_train_img_lst[int(CFG.split_pct*len(permuted_train_img_lst)) :]\n",
        "train_masks_list = permuted_train_mask_lst[int(CFG.split_pct*len(permuted_train_mask_lst)) :]\n",
        "\n",
        "val_images_list = permuted_train_img_lst[: int(CFG.split_pct*len(permuted_train_img_lst))]\n",
        "val_masks_list = permuted_train_mask_lst[: int(CFG.split_pct*len(permuted_train_mask_lst))]"
      ],
      "metadata": {
        "id": "lMEUfM1IFXJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self,img_list,mask_list,transform=None):\n",
        "        self.img_list = img_list\n",
        "        self.mask_list = mask_list\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        img_path = os.path.join(ROOTDIR.train,self.img_list[index])\n",
        "        mask_path = os.path.join(ROOTDIR.train_mask,self.mask_list[index])\n",
        "        img = Image.open(img_path)\n",
        "        mask = Image.open(mask_path)\n",
        "        mask = mask.convert(\"L\")\n",
        "        img = np.array(img)\n",
        "        mask = np.array(mask)\n",
        "        mask = mask / 255.0\n",
        "        #img_mask_dict = {\"image\": img, \"mask\": mask}\n",
        "        \n",
        "        if self.transform:\n",
        "            augmentation = self.transform(image=img, mask=mask)\n",
        "            img = augmentation[\"image\"]\n",
        "            mask = augmentation[\"mask\"]\n",
        "            mask = torch.unsqueeze(mask,0)\n",
        "            #transformations = self.transform(image=img, mask=mask)\n",
        "            #img = transformations[\"image\"]\n",
        "            #mask = transformations[\"mask\"]\n",
        "            \n",
        "        return img,mask"
      ],
      "metadata": {
        "id": "sMmtmZGZFddf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = A.Compose([A.Resize(224,224),\n",
        "                            A.Rotate(limit=75, p=0.9, border_mode = cv2.BORDER_REFLECT),\n",
        "                            A.RandomBrightnessContrast (brightness_limit=0.1, contrast_limit=0.1, brightness_by_max=True, always_apply=False, p=0.5), \n",
        "                            A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
        "                           ToTensorV2()])\n",
        "\n",
        "test_transform = A.Compose([A.Resize(224,224),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),                             \n",
        "                           ToTensorV2()])\n",
        "\n",
        "train_dataset = Dataset(train_images_list, train_masks_list, transform = train_transform)\n",
        "val_dataset = Dataset(val_images_list, val_masks_list, transform = test_transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,batch_size=CFG.batch_size,shuffle=True, num_workers=2)\n",
        "val_dataloader = DataLoader(val_dataset,batch_size=CFG.batch_size,shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "OqsFzwb2Fa79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,dataloader,criterion,optimizer):\n",
        "    model.train()\n",
        "    train_running_loss = 0.0\n",
        "    for j,img_mask in enumerate(tqdm(dataloader)):\n",
        "        img = img_mask[0].float().to(CFG.device)\n",
        "        #print(\" ----- IMAGE -----\")\n",
        "        #print(img)\n",
        "        mask = img_mask[1].float().to(CFG.device)\n",
        "        #print(\" ----- MASK -----\")\n",
        "        #print(mask)\n",
        "        \n",
        "        y_pred = model(img)\n",
        "        #print(\" ----- Y PRED -----\")\n",
        "        #print(y_pred)\n",
        "        #print(\" ----- Y PRED SHAPE -----\")#\n",
        "        #print(y_pred.shape)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss = criterion(y_pred,mask)\n",
        "        \n",
        "        train_running_loss += loss.item() * CFG.batch_size\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    train_loss = train_running_loss / (j+1)\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "\n",
        "def val_model(model,dataloader,criterion,optimizer):\n",
        "    model.eval()\n",
        "    val_running_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for j,img_mask in enumerate(tqdm(dataloader)):\n",
        "            img = img_mask[0].float().to(CFG.device)\n",
        "            mask = img_mask[1].float().to(CFG.device)\n",
        "            y_pred = model(img)\n",
        "            loss = criterion(y_pred,mask)\n",
        "            \n",
        "            val_running_loss += loss.item() * CFG.batch_size\n",
        "            \n",
        "        val_loss = val_running_loss / (j+1)\n",
        "    return val_loss"
      ],
      "metadata": {
        "id": "LWmUvFi0Fn0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = smp.UnetPlusPlus(\n",
        "    encoder_name=\"timm-mobilenetv3_large_100\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
        "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
        "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
        "    classes=1,\n",
        "    decoder_attention_type=\"scse\",\n",
        ")\n",
        "\n",
        "loss = smp.utils.losses.DiceLoss()\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "metrics = [smp.utils.metrics.IoU(threshold=0.5),]\n",
        "\n",
        "optimizer = torch.optim.Adam([dict(params=model.parameters(), lr=0.0001),])\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-4)"
      ],
      "metadata": {
        "id": "CZ8-XZe_GMVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_lst = [999]\n",
        "val_loss_lst = [999]\n",
        "\n",
        "\n",
        "for i in range(100):\n",
        "    train_loss = train_model(model=model,dataloader=train_dataloader,criterion=criterion,optimizer=optimizer)\n",
        "    val_loss = val_model(model=model,dataloader=val_dataloader,criterion=criterion,optimizer=optimizer)\n",
        "    print(val_loss)\n",
        "    print (\"Train Loss: \", train_loss_lst)\n",
        "    print(\"Val Loss: \", val_loss_lst)\n",
        "\n",
        "    lower = False\n",
        "\n",
        "    if val_loss < val_loss_lst[-1]: \n",
        "      lower = True\n",
        "      train_loss_lst.append(train_loss)\n",
        "      val_loss_lst.append(val_loss)\n",
        "    \n",
        "    if lower:\n",
        "      print(\"MODEL IMPROVED! ;)\")\n",
        "      torch.save(model.state_dict(), f\"/content/drive/MyDrive/seg_unet++_mobinetv3_size256x256_adam_{val_loss:.4f}.pth\")"
      ],
      "metadata": {
        "id": "hBzfwZkOHG4a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}